# Gemma 3 Standalone Chatbot

<details>
<summary><strong>ğŸ‡°ğŸ‡· Korean Version / í•œêµ­ì–´ ë²„ì „</strong></summary>

# Gemma 3 ìŠ¤íƒ ë“œì–¼ë¡  ì±—ë´‡

êµ¬ê¸€ì˜ **Gemma 3 1B** ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²½ëŸ‰ ìŠ¤íƒ ë“œì–¼ë¡  ì±—ë´‡ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” **FastAPI**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì„œë¹™í•˜ê³ , ì»¤ë§¨ë“œë¼ì¸ ì¸í„°í˜ì´ìŠ¤(CLI)ë¥¼ í†µí•´ ëª¨ë¸ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

**NVIDIA GPU (CUDA)** ë° **Apple Silicon (MPS)** í•˜ë“œì›¨ì–´ ê°€ì†ì„ ì§€ì›í•˜ë©°, ê°€ì†ê¸°ê°€ ì—†ëŠ” ê²½ìš° CPU ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.

## âœ¨ ì£¼ìš” ê¸°ëŠ¥

- **ë¡œì»¬ ì¸í¼ëŸ°ìŠ¤**: ì™¸ë¶€ API í˜¸ì¶œ ì—†ì´ ë¡œì»¬ ì¥ë¹„ì—ì„œ ì§ì ‘ ëª¨ë¸ì„ êµ¬ë™í•©ë‹ˆë‹¤.
- **RESTful API**: FastAPIë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¶•ëœ í™•ì¥ ê°€ëŠ¥í•œ ë°±ì—”ë“œ ì„œë²„ì…ë‹ˆë‹¤.
- **CLI í´ë¼ì´ì–¸íŠ¸**: ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í„°ë¯¸ë„ ê¸°ë°˜ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- **í•˜ë“œì›¨ì–´ ìµœì í™”**:
    - NVIDIA GPU (CUDA) ì§€ì›
    - Apple Silicon (MPS - Metal Performance Shaders) ì§€ì›
    - ìë™ ë””ë°”ì´ìŠ¤ ê°ì§€ ë° `float16` ì •ë°€ë„ ìµœì í™”
- **ëŒ€í™” ê¸°ì–µ**: ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ› ï¸ ì‚¬ì „ ìš”êµ¬ ì‚¬í•­

- **Python 3.10** ì´ìƒ
- **Hugging Face ê³„ì •** ë° **Access Token** (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš©)
- (ê¶Œì¥) NVIDIA GPU ë˜ëŠ” Apple Silicon Mac

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### 1. í™˜ê²½ ì„¤ì •

ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. ê°€ìƒ í™˜ê²½ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

```bash
# ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™” (ì˜ˆì‹œ)
python -m venv .venv
source .venv/bin/activate  # Mac/Linux
# .venv\Scripts\activate  # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. Hugging Face í† í° ì„¤ì •

`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  Hugging Face í† í°ì„ ì…ë ¥í•©ë‹ˆë‹¤. (ì°¸ê³ : `.env.example` íŒŒì¼ì´ ìˆë‹¤ë©´ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”)

```bash
# .env íŒŒì¼ ìƒì„±
echo "HUGGINGFACE_TOKEN=your_token_here" > .env
```

### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ Hugging Face Hubì—ì„œ Gemma 3 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

```bash
python download-model.py
```
> **ì°¸ê³ **: ëª¨ë¸ í¬ê¸°ëŠ” ì•½ 2~4GBì´ë©°, ì¸í„°ë„· ì†ë„ì— ë”°ë¼ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ’» ì‚¬ìš© ë°©ë²•

### 0. ì„œë²„ ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­
- ë‹¤ìš´ë¡œë“œ ëœ ê·¸ëŒ€ë¡œ ì´ìš©
    - modles/models--google--gemma-3-1b-it/snapshots/<HASH> í´ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
    - api.py íŒŒì¼ì˜ model_nameì„ "models/models--google--gemma-3-1b-it/snapshots/<HASH>"ë¡œ ë³€ê²½
- symbolic linkë¥¼ ì‹¤ì œ íŒŒì¼ë¡œ ë³€ê²½ í›„ ì´ìš©
    - modles/models--google--gemma-3-1b-it/snapshots/<HASH> í´ë”ì˜ symbolic linkë¥¼ ì‹¤ì œ íŒŒì¼ë¡œ ë³€ê²½ í›„, models/gemma3-1b-itë¡œ ë³€ê²½

### 1. API ì„œë²„ ì‹¤í–‰

ë°±ì—”ë“œ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.

```bash
./run.sh
# ë˜ëŠ”
uvicorn api:app --host 0.0.0.0 --port 8000 --reload
```
ì„œë²„ê°€ ì‹œì‘ë˜ë©´ `http://localhost:8000`ì—ì„œ ëŒ€ê¸°í•©ë‹ˆë‹¤.

### 2. ì±„íŒ… í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰

ìƒˆë¡œìš´ í„°ë¯¸ë„ ì°½ì„ ì—´ê³  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
python chat.py
```

### 3. API ë¬¸ì„œ

ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¼ ë•Œ ë¸Œë¼ìš°ì €ì—ì„œ ì•„ë˜ ì£¼ì†Œë¡œ ì ‘ì†í•˜ë©´ API ë¬¸ì„œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc)

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

- `api.py`: FastAPI ë°±ì—”ë“œ ì„œë²„ ë° ëª¨ë¸ ë¡œë”© ë¡œì§
- `chat.py`: ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” CLI í´ë¼ì´ì–¸íŠ¸
- `download-model.py`: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìœ í‹¸ë¦¬í‹°
- `run.sh`: ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- `models/`: ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ì´ ì €ì¥ë˜ëŠ” ë””ë ‰í† ë¦¬

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ì— ë”°ë¼ ë¼ì´ì„ ìŠ¤ê°€ ë¶€ê³¼ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤.

</details>

---

A lightweight, standalone chatbot application powered by Google's **Gemma 3 1B** model. This project demonstrates how to serve a Large Language Model (LLM) using **FastAPI** and interact with it via a command-line interface (CLI).

It supports hardware acceleration on **NVIDIA GPUs (CUDA)** and **Apple Silicon (MPS)**, falling back to CPU if neither is available.

---

## âœ¨ Features

- **Local Inference**: Runs entirely on your machine; no external API calls required after download.
- **RESTful API**: Scalable backend built with FastAPI.
- **CLI Client**: Easy-to-use terminal-based chat interface.
- **Hardware Optimization**:
    - Supports NVIDIA GPUs (CUDA).
    - Supports Apple Silicon (MPS - Metal Performance Shaders).
    - Automatic device detection and `float16` precision optimization.
- **Conversation History**: Manages context for multi-turn conversations.

## ğŸ› ï¸ Prerequisites

- **Python 3.10** or higher.
- **Hugging Face Account** and **Access Token** (to download the model).
- (Recommended) NVIDIA GPU or Apple Silicon Mac.

## ğŸš€ Installation & Setup

## ğŸ’» Usage

### 0. Before Running the Server
- **Use as downloaded**:
    - Verify that the `models/models--google--gemma-3-1b-it/snapshots/<HASH>` folder exists.
    - Update `model_name` in `api.py` to `"models/models--google--gemma-3-1b-it/snapshots/<HASH>"`.
- **Use after converting symbolic links to actual files**:
    - Replace symbolic links in `models/models--google--gemma-3-1b-it/snapshots/<HASH>` with actual files.
    - Rename the directory to `models/gemma3-1b-it`.


### 1. Environment Setup

Install the required packages. Using a virtual environment is recommended.

```bash
# Create and activate virtual environment (example)
python -m venv .venv
source .venv/bin/activate  # Mac/Linux
# .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Set Hugging Face Token

Create a `.env` file and add your Hugging Face token.

```bash
# Create .env file
echo "HUGGINGFACE_TOKEN=your_token_here" > .env
```

### 3. Download Model

Run the script to download the Gemma 3 model from Hugging Face Hub.

```bash
python download-model.py
```
> **Note**: The model is approximately 2-4GB. Download time depends on your internet connection.

## ğŸ’» Usage

### 1. Start the API Server

Launch the backend server.

```bash
./run.sh
# OR
uvicorn api:app --host 0.0.0.0 --port 8000 --reload
```
The server will start listening at `http://localhost:8000`.

### 2. Start the Chat Client

Open a new terminal window and run the client.

```bash
python chat.py
```

### 3. API Documentation

Once the server is running, you can access the interactive API documentation at:
- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc)

## ğŸ“ Project Structure

- `api.py`: FastAPI backend server and model loading logic.
- `chat.py`: CLI client for user interaction.
- `download-model.py`: Utility to download the model.
- `run.sh`: Server startup script.
- `models/`: Directory where the downloaded model is stored.

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
